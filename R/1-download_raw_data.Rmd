---
title: "Downloading CMIP6 data"
author: "Eric R. Scott"
date: "2021-11-10"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    number_sections: yes
    highlight: kate
    theme:
      version: 4
      bootswatch: flatly

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(epwshiftr)
library(glue)
library(lubridate)
```

*Last compiled: `r Sys.Date()`*

# Goal

Eventually, I'd like to download output data from all [CMIP6](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6) models running all 5 [SSP scenarios](https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change).  For the sake of developing this workflow, I'm going to work with two models and one scenario only.

# Get list of files to download

The `epwshiftr` package provides a function to query the CMIP6 database and get back a list of file URLs and metadata.

```{r}
idx_ssp126 <- init_cmip6_index(
  activity = "ScenarioMIP",
  variable = c("tas", "tasmin", "tasmax", "pr", "hfss", "hfls"),
  frequency = "mon",
  experiment = c("ssp126"),
  # source = , #leave this blank to get all models
  variant = "r1i1p1f1",
  years = 2015:2100, # this doesn't seem to work.  I'll filter manually
  save = TRUE
) %>%
  filter(datetime_end <= ymd("2100-12-01"))

head(idx_ssp126)
```

If I were to download all of these, what would be the total file size?

```{r}
idx_ssp126 %>% 
  summarize(total_size_GB = sum(file_size) * 9.31e-10,
            n_files = n())

idx_ssp126 %>% 
  group_by(variable_id, source_id) %>% 
  summarize(n_files = n())
```

Total of 11GB and 1694 .nc files just for ssp126!  Some models spread data across multiple small files, others just a single file.  Fortunately, I think it's not too difficult to combine multiple with `stars` if they have the same dimensions.

# Download

## Create directories

First, I set up directory structure using the index from above.

The path should be `/data_raw/experiment_id/source_id/`

```{r}
dirs <- here("data_raw", unique(idx_ssp126$experiment_id), unique(idx_ssp126$source_id))
#check if dirs already exist and only make ones that are missing
new_dirs <- dirs[!map_lgl(dirs, dir.exists)]
map(new_dirs, ~dir.create(.x, recursive = TRUE))
```

## Create file names

```{r}
idx_ssp126 <-
  idx_ssp126 %>% 
  mutate(filename = glue("{variable_id}_{experiment_id}_{source_id}_{datetime_start}_{datetime_end}.nc"),
         download_path = here("data_raw", experiment_id, source_id, filename))
```

## Download files

**TODO:** Wrap `download.file()` in a for-loop so only, say, 10 files are downloaded at a time with a `Sys.sleep()` in between.  Downloading too many files at once gets your IP temporarily blocked, I think.

For the sake of testing, I'm downloading files for two models---one that puts 2015--2100 into a single file and one that has a separate .nc file for each year.

```{r}
idx_ssp126 <- 
  idx_ssp126 %>%
  filter(source_id %in% c("MRI-ESM2-0", "EC-Earth3")) #just two models for example

#only download files that don't exist
to_get <- 
  idx_ssp126 %>%
  filter(!file.exists(download_path))

#temporary code to download files for each model separately
EC_earth <- to_get %>% 
  filter(source_id == "EC-Earth3")
MRI <- to_get %>% 
  filter(source_id == "MRI-ESM2-0")

if (nrow(MRI) > 0) {
  download.file(MRI$file_url, destfile = MRI$download_path)
}
# TODO: add some kind of check that download went well.  E.g. that file sizes match what's in idx_ssp126
```


```{r eval=FALSE}
#Not necessary to run this.  Haven't tested wrangling on this model yet, and it's a lot of files
if (nrow(EC_earth) > 0) {
  download.file(EC_earth$file_url, destfile = EC_earth$download_path)
}
```

